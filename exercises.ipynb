{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e0f4b22",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"logo-datakori.png\" width=\"400px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e13435a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68da3bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    FloatType,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bf0037",
   "metadata": {},
   "source": [
    "## 1 - Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acb317d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession est un point d'entrée pour Spark et depuis la version 2.0\n",
    "# de spark, il est indispensable d'en créer une instance dès les premières lignes\n",
    "# d'un programme spark pour pouvoir utiliser les RDD, DataFrame.\n",
    "\n",
    "\n",
    "# un attribut de classe ayant un constructeur pour créer une instance de SparkSession.\n",
    "# Permet de définir l'URL du noeud master  pour se connecter au cluster spark, such as \"local\"\n",
    "# pour utiliser une version locale de spark, \"local[4]\" une version locale avec 3 cores,\n",
    "# ou \"spark://master:7077\" pour s'exécuter sur un cluster Spark séparé.\n",
    "# donne un nom à notre application afin que nous puissions la visualiser dans Spark UI\n",
    "# Vérifie si une session spark globale existe déjà ou en crée une à défaut.\n",
    "\n",
    "spark: SparkSession = (\n",
    "    SparkSession.builder.master(\"local[*]\").appName(\"small-exercices\").getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f88369",
   "metadata": {},
   "source": [
    "## 2 - Create a dataframe for the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ca0d7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"James\", \"Sales\", 3000.0),\n",
    "    (2, \"Michael\", \"Sales\", 4600.0),\n",
    "    (3, \"Robert\", \"Sales\", 4100.0),\n",
    "    (4, \"Maria\", \"Finance\", 3000.5),\n",
    "    (5, \"James\", \"Sales\", 3000.5),\n",
    "    (6, \"Scott\", \"Finance\", 3300.0),\n",
    "    (7, \"Jen\", \"Finance\", 3900.0),\n",
    "    (8, \"Jena\", \"Finance\", 3900.0),\n",
    "    (9, \"Jeff\", \"Marketing\", 3000.0),\n",
    "    (10, \"Kumar\", \"Marketing\", 2000.0),\n",
    "    (11, \"Kumar\", \"Marketing\", 2000.0),\n",
    "    (12, \"Saif\", \"Sales\", 4100.0),\n",
    "]\n",
    "\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", IntegerType(), False),\n",
    "        StructField(\"employee_name\", StringType(), True),\n",
    "        StructField(\"department\", StringType(), True),\n",
    "        StructField(\"salary\", FloatType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c9770fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------+------+\n",
      "|id |employee_name|department|salary|\n",
      "+---+-------------+----------+------+\n",
      "|1  |James        |Sales     |3000.0|\n",
      "|2  |Michael      |Sales     |4600.0|\n",
      "|3  |Robert       |Sales     |4100.0|\n",
      "|4  |Maria        |Finance   |3000.5|\n",
      "|5  |James        |Sales     |3000.5|\n",
      "|6  |Scott        |Finance   |3300.0|\n",
      "|7  |Jen          |Finance   |3900.0|\n",
      "|8  |Jena         |Finance   |3900.0|\n",
      "|9  |Jeff         |Marketing |3000.0|\n",
      "|10 |Kumar        |Marketing |2000.0|\n",
      "|11 |Kumar        |Marketing |2000.0|\n",
      "|12 |Saif         |Sales     |4100.0|\n",
      "+---+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fe136d",
   "metadata": {},
   "source": [
    "## 3 - Drop duplicated lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5db456fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000.0|\n",
      "|Michael      |Sales     |4600.0|\n",
      "|Robert       |Sales     |4100.0|\n",
      "|Maria        |Finance   |3000.5|\n",
      "|Scott        |Finance   |3300.0|\n",
      "|James        |Sales     |3000.5|\n",
      "|Jen          |Finance   |3900.0|\n",
      "|Jeff         |Marketing |3000.0|\n",
      "|Jena         |Finance   |3900.0|\n",
      "|Kumar        |Marketing |2000.0|\n",
      "|Saif         |Sales     |4100.0|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a temporay view that will last as long as the SparkSession\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# In SQL\n",
    "request = \"\"\"\n",
    "    SELECT employee_name, department, salary\n",
    "    FROM people\n",
    "    GROUP BY employee_name, department, salary\n",
    "\"\"\"\n",
    "spark.sql(request).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "893245f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000.0|\n",
      "|Michael      |Sales     |4600.0|\n",
      "|Robert       |Sales     |4100.0|\n",
      "|Maria        |Finance   |3000.5|\n",
      "|Scott        |Finance   |3300.0|\n",
      "|James        |Sales     |3000.5|\n",
      "|Jen          |Finance   |3900.0|\n",
      "|Jena         |Finance   |3900.0|\n",
      "|Jeff         |Marketing |3000.0|\n",
      "|Kumar        |Marketing |2000.0|\n",
      "|Saif         |Sales     |4100.0|\n",
      "|Kumar        |Marketing |2000.0|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In spark\n",
    "df = df.drop_duplicates()\n",
    "df.select(\"employee_name\", \"department\", \"salary\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534d52c6",
   "metadata": {},
   "source": [
    "## 4 - Cummulative sum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a702f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-------+\n",
      "|employee_name|department|sum    |\n",
      "+-------------+----------+-------+\n",
      "|Maria        |Finance   |3000.5 |\n",
      "|Scott        |Finance   |6300.5 |\n",
      "|Jen          |Finance   |10200.5|\n",
      "|Jena         |Finance   |14100.5|\n",
      "|Kumar        |Marketing |2000.0 |\n",
      "|Kumar        |Marketing |4000.0 |\n",
      "|Jeff         |Marketing |7000.0 |\n",
      "|James        |Sales     |3000.0 |\n",
      "|James        |Sales     |6000.5 |\n",
      "|Robert       |Sales     |10100.5|\n",
      "|Saif         |Sales     |14200.5|\n",
      "|Michael      |Sales     |18800.5|\n",
      "+-------------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df.select(\n",
    "    \"employee_name\",\n",
    "    \"department\",\n",
    "    f.sum(\"salary\").over(windowSpec.rowsBetween(-sys.maxsize, 0)).alias(\"sum\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c5f154b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-------+\n",
      "|employee_name|department|salary|    sum|\n",
      "+-------------+----------+------+-------+\n",
      "|        Maria|   Finance|3000.5| 3000.5|\n",
      "|        Scott|   Finance|3300.0| 6300.5|\n",
      "|          Jen|   Finance|3900.0|10200.5|\n",
      "|         Jena|   Finance|3900.0|14100.5|\n",
      "|        Kumar| Marketing|2000.0| 2000.0|\n",
      "|        Kumar| Marketing|2000.0| 4000.0|\n",
      "|         Jeff| Marketing|3000.0| 7000.0|\n",
      "|        James|     Sales|3000.0| 3000.0|\n",
      "|        James|     Sales|3000.5| 6000.5|\n",
      "|       Robert|     Sales|4100.0|10100.5|\n",
      "|         Saif|     Sales|4100.0|14200.5|\n",
      "|      Michael|     Sales|4600.0|18800.5|\n",
      "+-------------+----------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "request = \"\"\"\n",
    "    SELECT employee_name, department, salary,\n",
    "        SUM(salary) OVER(PARTITION BY department ORDER BY salary ROWS BETWEEN unbounded preceding AND CURRENT ROW) AS sum  \n",
    "    FROM people\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "spark.sql(request).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f412f62",
   "metadata": {},
   "source": [
    "## 5 - Row number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19c9d774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------+------+----------+\n",
      "|id |employee_name|department|salary|row_number|\n",
      "+---+-------------+----------+------+----------+\n",
      "|7  |Jen          |Finance   |3900.0|1         |\n",
      "|8  |Jena         |Finance   |3900.0|2         |\n",
      "|6  |Scott        |Finance   |3300.0|3         |\n",
      "|4  |Maria        |Finance   |3000.5|4         |\n",
      "|9  |Jeff         |Marketing |3000.0|1         |\n",
      "|10 |Kumar        |Marketing |2000.0|2         |\n",
      "|11 |Kumar        |Marketing |2000.0|3         |\n",
      "|2  |Michael      |Sales     |4600.0|1         |\n",
      "|3  |Robert       |Sales     |4100.0|2         |\n",
      "|12 |Saif         |Sales     |4100.0|3         |\n",
      "|5  |James        |Sales     |3000.5|4         |\n",
      "|1  |James        |Sales     |3000.0|5         |\n",
      "+---+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Return a sequential number starting at 1 within a window partion\n",
    "# In spark\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(f.col(\"salary\").desc())\n",
    "\n",
    "df.withColumn(\"row_number\", f.row_number().over(windowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5acac8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+---------+\n",
      "|employee_name|department|rowNumber|\n",
      "+-------------+----------+---------+\n",
      "|        Maria|   Finance|        1|\n",
      "|        Scott|   Finance|        2|\n",
      "|          Jen|   Finance|        3|\n",
      "|         Jena|   Finance|        4|\n",
      "|        Kumar| Marketing|        1|\n",
      "|        Kumar| Marketing|        2|\n",
      "|         Jeff| Marketing|        3|\n",
      "|        James|     Sales|        1|\n",
      "|        James|     Sales|        2|\n",
      "|       Robert|     Sales|        3|\n",
      "|         Saif|     Sales|        4|\n",
      "|      Michael|     Sales|        5|\n",
      "+-------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In SQL\n",
    "request = \"\"\"\n",
    "    SELECT employee_name, department, \n",
    "        ROW_NUMBER() OVER(PARTITION BY department ORDER BY salary) AS rowNumber\n",
    "    FROM people\n",
    "\"\"\"\n",
    "spark.sql(request).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c053b41a",
   "metadata": {},
   "source": [
    "## 6 - Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27c06ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------+------+-----+\n",
      "|id |employee_name|department|salary|ranks|\n",
      "+---+-------------+----------+------+-----+\n",
      "|4  |Maria        |Finance   |3000.5|1    |\n",
      "|6  |Scott        |Finance   |3300.0|2    |\n",
      "|7  |Jen          |Finance   |3900.0|3    |\n",
      "|8  |Jena         |Finance   |3900.0|3    |\n",
      "|10 |Kumar        |Marketing |2000.0|1    |\n",
      "|11 |Kumar        |Marketing |2000.0|1    |\n",
      "|9  |Jeff         |Marketing |3000.0|3    |\n",
      "|1  |James        |Sales     |3000.0|1    |\n",
      "|5  |James        |Sales     |3000.5|2    |\n",
      "|3  |Robert       |Sales     |4100.0|3    |\n",
      "|12 |Saif         |Sales     |4100.0|3    |\n",
      "|2  |Michael      |Sales     |4600.0|5    |\n",
      "+---+-------------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The difference between rank and dense_rank is that dense_rank leaves \n",
    "# no gaps in ranking sequence when there are ties\n",
    "\n",
    "# In spark\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df.withColumn(\"ranks\", f.rank().over(windowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06783f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----+\n",
      "|employee_name|department|salary|ranks|\n",
      "+-------------+----------+------+-----+\n",
      "|        Maria|   Finance|3000.5|    1|\n",
      "|        Scott|   Finance|3300.0|    2|\n",
      "|          Jen|   Finance|3900.0|    3|\n",
      "|         Jena|   Finance|3900.0|    3|\n",
      "|        Kumar| Marketing|2000.0|    1|\n",
      "|        Kumar| Marketing|2000.0|    1|\n",
      "|         Jeff| Marketing|3000.0|    3|\n",
      "|        James|     Sales|3000.0|    1|\n",
      "|        James|     Sales|3000.5|    2|\n",
      "|       Robert|     Sales|4100.0|    3|\n",
      "|         Saif|     Sales|4100.0|    3|\n",
      "|      Michael|     Sales|4600.0|    5|\n",
      "+-------------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In SQL\n",
    "request = \"\"\"\n",
    "    SELECT employee_name, department, salary,\n",
    "        RANK() OVER(PARTITION BY department ORDER BY salary) AS ranks\n",
    "    FROM people\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(request).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f5e03d",
   "metadata": {},
   "source": [
    "# 7 - Dense rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ae4da99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------+------+-----------+\n",
      "|id |employee_name|department|salary|dense_ranks|\n",
      "+---+-------------+----------+------+-----------+\n",
      "|4  |Maria        |Finance   |3000.5|1          |\n",
      "|6  |Scott        |Finance   |3300.0|2          |\n",
      "|7  |Jen          |Finance   |3900.0|3          |\n",
      "|8  |Jena         |Finance   |3900.0|3          |\n",
      "|10 |Kumar        |Marketing |2000.0|1          |\n",
      "|11 |Kumar        |Marketing |2000.0|1          |\n",
      "|9  |Jeff         |Marketing |3000.0|2          |\n",
      "|1  |James        |Sales     |3000.0|1          |\n",
      "|5  |James        |Sales     |3000.5|2          |\n",
      "|3  |Robert       |Sales     |4100.0|3          |\n",
      "|12 |Saif         |Sales     |4100.0|3          |\n",
      "|2  |Michael      |Sales     |4600.0|4          |\n",
      "+---+-------------+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In spark\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df.withColumn(\"dense_ranks\", f.dense_rank().over(windowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97085c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----------+\n",
      "|employee_name|department|salary|dense_ranks|\n",
      "+-------------+----------+------+-----------+\n",
      "|Maria        |Finance   |3000.5|1          |\n",
      "|Scott        |Finance   |3300.0|2          |\n",
      "|Jen          |Finance   |3900.0|3          |\n",
      "|Jena         |Finance   |3900.0|3          |\n",
      "|Kumar        |Marketing |2000.0|1          |\n",
      "|Kumar        |Marketing |2000.0|1          |\n",
      "|Jeff         |Marketing |3000.0|2          |\n",
      "|James        |Sales     |3000.0|1          |\n",
      "|James        |Sales     |3000.5|2          |\n",
      "|Robert       |Sales     |4100.0|3          |\n",
      "|Saif         |Sales     |4100.0|3          |\n",
      "|Michael      |Sales     |4600.0|4          |\n",
      "+-------------+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In SQL\n",
    "request = \"\"\"\n",
    "    SELECT employee_name, department, salary, \n",
    "        DENSE_RANK() OVER(PARTITION BY department ORDER BY salary) AS dense_ranks\n",
    "    FROM people\n",
    "\"\"\"\n",
    "spark.sql(request).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aed3ead",
   "metadata": {},
   "source": [
    "# 8 - Percent rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6c0a05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------+------+------------------+\n",
      "|id |employee_name|department|salary|percent_ranks     |\n",
      "+---+-------------+----------+------+------------------+\n",
      "|4  |Maria        |Finance   |3000.5|0.0               |\n",
      "|6  |Scott        |Finance   |3300.0|0.3333333333333333|\n",
      "|7  |Jen          |Finance   |3900.0|0.6666666666666666|\n",
      "|8  |Jena         |Finance   |3900.0|0.6666666666666666|\n",
      "|10 |Kumar        |Marketing |2000.0|0.0               |\n",
      "|11 |Kumar        |Marketing |2000.0|0.0               |\n",
      "|9  |Jeff         |Marketing |3000.0|1.0               |\n",
      "|1  |James        |Sales     |3000.0|0.0               |\n",
      "|5  |James        |Sales     |3000.5|0.25              |\n",
      "|3  |Robert       |Sales     |4100.0|0.5               |\n",
      "|12 |Saif         |Sales     |4100.0|0.5               |\n",
      "|2  |Michael      |Sales     |4600.0|1.0               |\n",
      "+---+-------------+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# returns the relative rank (i.e. percentile) of rows within a window partition\n",
    "# In spark\n",
    "df.withColumn(\"percent_ranks\", f.percent_rank().over(windowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f092b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------------+\n",
      "|employee_name|department|salary|percent_ranks     |\n",
      "+-------------+----------+------+------------------+\n",
      "|Maria        |Finance   |3000.5|0.0               |\n",
      "|Scott        |Finance   |3300.0|0.3333333333333333|\n",
      "|Jen          |Finance   |3900.0|0.6666666666666666|\n",
      "|Jena         |Finance   |3900.0|0.6666666666666666|\n",
      "|Kumar        |Marketing |2000.0|0.0               |\n",
      "|Kumar        |Marketing |2000.0|0.0               |\n",
      "|Jeff         |Marketing |3000.0|1.0               |\n",
      "|James        |Sales     |3000.0|0.0               |\n",
      "|James        |Sales     |3000.5|0.25              |\n",
      "|Robert       |Sales     |4100.0|0.5               |\n",
      "|Saif         |Sales     |4100.0|0.5               |\n",
      "|Michael      |Sales     |4600.0|1.0               |\n",
      "+-------------+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In SQL\n",
    "request = \"\"\"\n",
    "    SELECT employee_name, department, salary, \n",
    "        PERCENT_RANK() OVER(PARTITION BY department ORDER BY salary) AS percent_ranks\n",
    "    FROM people\n",
    "\"\"\"\n",
    "spark.sql(request).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811385c0",
   "metadata": {},
   "source": [
    "# 9 - Ntile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80d62c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------+------+------+\n",
      "|id |employee_name|department|salary|ntiles|\n",
      "+---+-------------+----------+------+------+\n",
      "|4  |Maria        |Finance   |3000.5|1     |\n",
      "|6  |Scott        |Finance   |3300.0|2     |\n",
      "|7  |Jen          |Finance   |3900.0|3     |\n",
      "|8  |Jena         |Finance   |3900.0|4     |\n",
      "|10 |Kumar        |Marketing |2000.0|1     |\n",
      "|11 |Kumar        |Marketing |2000.0|2     |\n",
      "|9  |Jeff         |Marketing |3000.0|3     |\n",
      "|1  |James        |Sales     |3000.0|1     |\n",
      "|5  |James        |Sales     |3000.5|1     |\n",
      "|3  |Robert       |Sales     |4100.0|2     |\n",
      "|12 |Saif         |Sales     |4100.0|3     |\n",
      "|2  |Michael      |Sales     |4600.0|4     |\n",
      "+---+-------------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if n is 4, the first quarter of the rows will get value 1, the second quarter will get 2,\n",
    "# the third quarter will get 3, and the last quarter will get 4.\n",
    "# In spark\n",
    "df.withColumn(\"ntiles\", f.ntile(4).over(windowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8b45119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------+\n",
      "|employee_name|department|salary|ntiles|\n",
      "+-------------+----------+------+------+\n",
      "|Maria        |Finance   |3000.5|1     |\n",
      "|Scott        |Finance   |3300.0|2     |\n",
      "|Jen          |Finance   |3900.0|3     |\n",
      "|Jena         |Finance   |3900.0|4     |\n",
      "|Kumar        |Marketing |2000.0|1     |\n",
      "|Kumar        |Marketing |2000.0|2     |\n",
      "|Jeff         |Marketing |3000.0|3     |\n",
      "|James        |Sales     |3000.0|1     |\n",
      "|James        |Sales     |3000.5|1     |\n",
      "|Robert       |Sales     |4100.0|2     |\n",
      "|Saif         |Sales     |4100.0|3     |\n",
      "|Michael      |Sales     |4600.0|4     |\n",
      "+-------------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In SQL\n",
    "request = \"\"\"\n",
    "    SELECT employee_name, department, salary, \n",
    "        NTILE(4) OVER(PARTITION BY department ORDER BY salary) AS ntiles\n",
    "    FROM people\n",
    "\"\"\"\n",
    "spark.sql(request).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1e5dcf",
   "metadata": {},
   "source": [
    "## 10 - Lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e2e1506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------+------+----------+\n",
      "| id|employee_name|department|salary|lag_salary|\n",
      "+---+-------------+----------+------+----------+\n",
      "|  4|        Maria|   Finance|3000.5|      null|\n",
      "|  6|        Scott|   Finance|3300.0|    3000.5|\n",
      "|  7|          Jen|   Finance|3900.0|    3300.0|\n",
      "|  8|         Jena|   Finance|3900.0|    3900.0|\n",
      "| 10|        Kumar| Marketing|2000.0|      null|\n",
      "| 11|        Kumar| Marketing|2000.0|    2000.0|\n",
      "|  9|         Jeff| Marketing|3000.0|    2000.0|\n",
      "|  1|        James|     Sales|3000.0|      null|\n",
      "|  5|        James|     Sales|3000.5|    3000.0|\n",
      "|  3|       Robert|     Sales|4100.0|    3000.5|\n",
      "| 12|         Saif|     Sales|4100.0|    4100.0|\n",
      "|  2|      Michael|     Sales|4600.0|    4100.0|\n",
      "+---+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lag window function\n",
    "df.withColumn(\"lag_salary\", f.lag(\"salary\", 1).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9770cf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------+\n",
      "|employee_name|department|salary|lags  |\n",
      "+-------------+----------+------+------+\n",
      "|Maria        |Finance   |3000.5|null  |\n",
      "|Scott        |Finance   |3300.0|3000.5|\n",
      "|Jen          |Finance   |3900.0|3300.0|\n",
      "|Jena         |Finance   |3900.0|3900.0|\n",
      "|Kumar        |Marketing |2000.0|null  |\n",
      "|Kumar        |Marketing |2000.0|2000.0|\n",
      "|Jeff         |Marketing |3000.0|2000.0|\n",
      "|James        |Sales     |3000.0|null  |\n",
      "|James        |Sales     |3000.5|3000.0|\n",
      "|Robert       |Sales     |4100.0|3000.5|\n",
      "|Saif         |Sales     |4100.0|4100.0|\n",
      "|Michael      |Sales     |4600.0|4100.0|\n",
      "+-------------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lag in sql\n",
    "request = \"\"\"\n",
    "    SELECT employee_name, department, salary, \n",
    "        LAG(salary) OVER(PARTITION BY department ORDER BY salary) AS lags\n",
    "    FROM people\n",
    "\"\"\"\n",
    "spark.sql(request).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbb7ee0",
   "metadata": {},
   "source": [
    "# 11 - Lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d329af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------+------+-----------+\n",
      "|id |employee_name|department|salary|lead_salary|\n",
      "+---+-------------+----------+------+-----------+\n",
      "|4  |Maria        |Finance   |3000.5|3300.0     |\n",
      "|6  |Scott        |Finance   |3300.0|3900.0     |\n",
      "|7  |Jen          |Finance   |3900.0|3900.0     |\n",
      "|8  |Jena         |Finance   |3900.0|null       |\n",
      "|10 |Kumar        |Marketing |2000.0|2000.0     |\n",
      "|11 |Kumar        |Marketing |2000.0|3000.0     |\n",
      "|9  |Jeff         |Marketing |3000.0|null       |\n",
      "|1  |James        |Sales     |3000.0|3000.5     |\n",
      "|5  |James        |Sales     |3000.5|4100.0     |\n",
      "|3  |Robert       |Sales     |4100.0|4100.0     |\n",
      "|12 |Saif         |Sales     |4100.0|4600.0     |\n",
      "|2  |Michael      |Sales     |4600.0|null       |\n",
      "+---+-------------+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lead window function\n",
    "df.withColumn(\"lead_salary\", f.lead(\"salary\", 1).over(windowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16152cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------+\n",
      "|employee_name|department|salary|leads |\n",
      "+-------------+----------+------+------+\n",
      "|Maria        |Finance   |3000.5|3300.0|\n",
      "|Scott        |Finance   |3300.0|3900.0|\n",
      "|Jen          |Finance   |3900.0|3900.0|\n",
      "|Jena         |Finance   |3900.0|null  |\n",
      "|Kumar        |Marketing |2000.0|2000.0|\n",
      "|Kumar        |Marketing |2000.0|3000.0|\n",
      "|Jeff         |Marketing |3000.0|null  |\n",
      "|James        |Sales     |3000.0|3000.5|\n",
      "|James        |Sales     |3000.5|4100.0|\n",
      "|Robert       |Sales     |4100.0|4100.0|\n",
      "|Saif         |Sales     |4100.0|4600.0|\n",
      "|Michael      |Sales     |4600.0|null  |\n",
      "+-------------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lead in sql\n",
    "request = \"\"\"\n",
    "    SELECT employee_name, department, salary, \n",
    "        LEAD(salary) OVER(PARTITION BY department ORDER BY salary) AS leads\n",
    "    FROM people\n",
    "\"\"\"\n",
    "spark.sql(request).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26650d25",
   "metadata": {},
   "source": [
    "## 12 - Bonus: replace a value by another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2da213ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------+------+--------------+\n",
      "|id |employee_name|department|salary|new_department|\n",
      "+---+-------------+----------+------+--------------+\n",
      "|1  |James        |Sales     |3000.0|Dir. Com.     |\n",
      "|2  |Michael      |Sales     |4600.0|Dir. Com.     |\n",
      "|3  |Robert       |Sales     |4100.0|Dir. Com.     |\n",
      "|4  |Maria        |Finance   |3000.5|Dir. Fin.     |\n",
      "|6  |Scott        |Finance   |3300.0|Dir. Fin.     |\n",
      "|5  |James        |Sales     |3000.5|Dir. Com.     |\n",
      "|7  |Jen          |Finance   |3900.0|Dir. Fin.     |\n",
      "|8  |Jena         |Finance   |3900.0|Dir. Fin.     |\n",
      "|9  |Jeff         |Marketing |3000.0|Dir. Mark.    |\n",
      "|10 |Kumar        |Marketing |2000.0|Dir. Mark.    |\n",
      "|12 |Saif         |Sales     |4100.0|Dir. Com.     |\n",
      "|11 |Kumar        |Marketing |2000.0|Dir. Mark.    |\n",
      "+---+-------------+----------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace Sales by Dir Com., Finance by Dir Fin., Marketing by Dir Mark. pyspark\n",
    "\n",
    "df.withColumn(\n",
    "    \"new_department\",\n",
    "    f.when(f.col(\"department\") == \"Sales\", \"Dir. Com.\")\n",
    "    .when(f.col(\"department\") == \"Finance\", \"Dir. Fin.\")\n",
    "    .otherwise(\"Dir. Mark.\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78dec36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|new_dept  |\n",
      "+-------------+----------+------+----------+\n",
      "|James        |Sales     |3000.0|Dir. Com. |\n",
      "|Michael      |Sales     |4600.0|Dir. Com. |\n",
      "|Robert       |Sales     |4100.0|Dir. Com. |\n",
      "|Maria        |Finance   |3000.5|Dir. Fin. |\n",
      "|James        |Sales     |3000.5|Dir. Com. |\n",
      "|Scott        |Finance   |3300.0|Dir. Fin. |\n",
      "|Jen          |Finance   |3900.0|Dir. Fin. |\n",
      "|Jena         |Finance   |3900.0|Dir. Fin. |\n",
      "|Jeff         |Marketing |3000.0|Dir. Mark.|\n",
      "|Kumar        |Marketing |2000.0|Dir. Mark.|\n",
      "|Kumar        |Marketing |2000.0|Dir. Mark.|\n",
      "|Saif         |Sales     |4100.0|Dir. Com. |\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace Sales by Dir Com., Finance by Dir Fin., Marketing by Dir Mark. SQL\n",
    "request = \"\"\"\n",
    "    SELECT employee_name, department, salary,\n",
    "        CASE\n",
    "            WHEN department = 'Sales' THEN 'Dir. Com.' \n",
    "            WHEN department = 'Finance' THEN 'Dir. Fin.' \n",
    "            ELSE 'Dir. Mark.'\n",
    "        END AS new_dept\n",
    "    FROM people\n",
    "\"\"\"\n",
    "spark.sql(request).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce55622",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
